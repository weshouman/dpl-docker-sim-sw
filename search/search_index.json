{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Index \u00b6 A Unified Approach for Simulation Software Development in Isolated Environments \u00b6 This documentation discusses methodologies for unifying the development of simulation software, following the Simulation Software Engineering Course at Stuttgart University. There are 3 different ways for how the containerization would fit and help in the process of developing research/simulation software, namely; building the software, developing the software, and using the software, the 3 different forms are described in the following sections in details. Build Process Isolation Software Environment Isolation Notes \u00b6 Throughout the documentation, the containers were preferred over the virtual machines, as the increased level of virtualization won\u2019t be worth the penalty on the developer (penalty of the storage, processing, memory and time increase) same goes for lxc (even with it being much lighter than the virtual machines). Integration into Kubernetes clusters is also a great bonus for container usage.","title":"Index"},{"location":"#index","text":"","title":"Index"},{"location":"#a-unified-approach-for-simulation-software-development-in-isolated-environments","text":"This documentation discusses methodologies for unifying the development of simulation software, following the Simulation Software Engineering Course at Stuttgart University. There are 3 different ways for how the containerization would fit and help in the process of developing research/simulation software, namely; building the software, developing the software, and using the software, the 3 different forms are described in the following sections in details. Build Process Isolation Software Environment Isolation","title":"A Unified Approach for Simulation Software Development in Isolated Environments"},{"location":"#notes","text":"Throughout the documentation, the containers were preferred over the virtual machines, as the increased level of virtualization won\u2019t be worth the penalty on the developer (penalty of the storage, processing, memory and time increase) same goes for lxc (even with it being much lighter than the virtual machines). Integration into Kubernetes clusters is also a great bonus for container usage.","title":"Notes"},{"location":"build-process-isolation/","text":"Build Process Isolation \u00b6 Build process isolation is needed for 2 reasons, ensuring that the master is building, and having an isolated environment for development. Pros \u00b6 Lots of varieties and possibilities are now open, including but not limited to: Using a Kubernetes cluster of multiple nodes: The build could be parallelized for an increased speed (of course if the SW supports parallelization and the parallelization between nodes are not expensive) Using a Kubernetes cluster of multiple heterogeneous nodes: The build could be tested on different architectures, for example x64 and arm. Using Jenkins: It\u2019s easily executable on other machines than the one in use Software Building \u00b6 Software that has been pushed to the remote, have to be successfully building. Lots of SW provides a HowTo for the build process, however it\u2019s not always the case that the build process is easily re-runnable for everybody. Here the build process is to be written into a Dockerfile so that it could run isolated by building its image. The verdict of whether the build passed or failed would be the return of 0 or may be even more serious by running some acceptance tests. Note: It\u2019s desired to avoid recreating big chunks while building the container image by utilizing the docker image caching and minimizing the effort done through each step, for example by using a separate apt install or pacman -S per package when suitable, and of course separating the build in a different step then dependency capturing. For example: Avoid using RUN apt update && \\ apt install a b -y && \\ ./configure && \\ make in favor of using RUN apt update RUN apt install a -y RUN apt install b -y RUN ./configure RUN make Examples \u00b6 SW Source Capturing Issues Caution fixes Status deal2 AUR Done dune Done dumux Done eigen Arch + git repo Done espresso AUR Python-sphinx<2.1.0 is not found Building fails, follow https://aur.archlinux.org/packages/espressomd-git#comment-886320 cuda is to be installed, consuming 1.8GB of bandwidth and 6GB of disk space While installing, we had to correct the sha256sum of python-griddataformats in the aur Failed fenics Building fenics has some issues issue with build and install the C++ core \u00b7 Issue #2410 \u00b7 FEniCS/dolfinx \u00b7 GitHub Failed firedrake Ubuntu, build script https://github.com/firedrakeproject/firedrake/discussions/2596 Done gmsh AUR Done lammps Done palabos hdf5-openmpi was chosen for the installation https://gitlab.com/unigespc/palabos/-/issues/110, ssh is necessary for the first example Dependency on SSH is not stated (#111) \u00b7 Issues \u00b7 UniGeSPC / palabos \u00b7 GitLab Done petsc AUR Done precice AUR Done pyiron pymor su2 sundials trilinos visit xsdk ginkgo Software Development \u00b6 For the case of having the source code locally and desiring to build it while developing it or just after doing some modifications it\u2019s beneficial to containerize the build process that captures uncommitted changes. With such isolation the developer could easily try the changes on different SW versions which in turn could have different dependencies, or may even try the changes on the same SW version but on different Linux flavors. Thus increasing the ability of observing the extent of the changes. Notes \u00b6 Ansible was avoided for the build steps in the implemented examples on purpose to maximize the cache usage, and speed up prototyping, however in more advanced scenarios especially when a hybrid situation of VM and container are to be deployed onto, an Ansible playbook would be very useful. Further caching optimizations could be considered by comparing the dependency sizes/time for building and reusing some those images, that would be useful if different simulation software are expected to run on the same machine. Comparison \u00b6 The main differences between the Development and the Building scenarios are in the following aspects. Caching \u00b6 For the Development scenarios caching previous builds needs to be considered, otherwise, one would spend a lot of time only doing superfluous rebuilds. Context \u00b6 For the Development scenarios we need to have a context, for example the source code should be available on our machine, which would be shared of course with the container, however, ideally, such context is not mandatory for Building scenarios.","title":"Build Process Isolation"},{"location":"build-process-isolation/#build-process-isolation","text":"Build process isolation is needed for 2 reasons, ensuring that the master is building, and having an isolated environment for development.","title":"Build Process Isolation"},{"location":"build-process-isolation/#pros","text":"Lots of varieties and possibilities are now open, including but not limited to: Using a Kubernetes cluster of multiple nodes: The build could be parallelized for an increased speed (of course if the SW supports parallelization and the parallelization between nodes are not expensive) Using a Kubernetes cluster of multiple heterogeneous nodes: The build could be tested on different architectures, for example x64 and arm. Using Jenkins: It\u2019s easily executable on other machines than the one in use","title":"Pros"},{"location":"build-process-isolation/#software-building","text":"Software that has been pushed to the remote, have to be successfully building. Lots of SW provides a HowTo for the build process, however it\u2019s not always the case that the build process is easily re-runnable for everybody. Here the build process is to be written into a Dockerfile so that it could run isolated by building its image. The verdict of whether the build passed or failed would be the return of 0 or may be even more serious by running some acceptance tests. Note: It\u2019s desired to avoid recreating big chunks while building the container image by utilizing the docker image caching and minimizing the effort done through each step, for example by using a separate apt install or pacman -S per package when suitable, and of course separating the build in a different step then dependency capturing. For example: Avoid using RUN apt update && \\ apt install a b -y && \\ ./configure && \\ make in favor of using RUN apt update RUN apt install a -y RUN apt install b -y RUN ./configure RUN make","title":"Software Building"},{"location":"build-process-isolation/#examples","text":"SW Source Capturing Issues Caution fixes Status deal2 AUR Done dune Done dumux Done eigen Arch + git repo Done espresso AUR Python-sphinx<2.1.0 is not found Building fails, follow https://aur.archlinux.org/packages/espressomd-git#comment-886320 cuda is to be installed, consuming 1.8GB of bandwidth and 6GB of disk space While installing, we had to correct the sha256sum of python-griddataformats in the aur Failed fenics Building fenics has some issues issue with build and install the C++ core \u00b7 Issue #2410 \u00b7 FEniCS/dolfinx \u00b7 GitHub Failed firedrake Ubuntu, build script https://github.com/firedrakeproject/firedrake/discussions/2596 Done gmsh AUR Done lammps Done palabos hdf5-openmpi was chosen for the installation https://gitlab.com/unigespc/palabos/-/issues/110, ssh is necessary for the first example Dependency on SSH is not stated (#111) \u00b7 Issues \u00b7 UniGeSPC / palabos \u00b7 GitLab Done petsc AUR Done precice AUR Done pyiron pymor su2 sundials trilinos visit xsdk ginkgo","title":"Examples"},{"location":"build-process-isolation/#software-development","text":"For the case of having the source code locally and desiring to build it while developing it or just after doing some modifications it\u2019s beneficial to containerize the build process that captures uncommitted changes. With such isolation the developer could easily try the changes on different SW versions which in turn could have different dependencies, or may even try the changes on the same SW version but on different Linux flavors. Thus increasing the ability of observing the extent of the changes.","title":"Software Development"},{"location":"build-process-isolation/#notes","text":"Ansible was avoided for the build steps in the implemented examples on purpose to maximize the cache usage, and speed up prototyping, however in more advanced scenarios especially when a hybrid situation of VM and container are to be deployed onto, an Ansible playbook would be very useful. Further caching optimizations could be considered by comparing the dependency sizes/time for building and reusing some those images, that would be useful if different simulation software are expected to run on the same machine.","title":"Notes"},{"location":"build-process-isolation/#comparison","text":"The main differences between the Development and the Building scenarios are in the following aspects.","title":"Comparison"},{"location":"build-process-isolation/#caching","text":"For the Development scenarios caching previous builds needs to be considered, otherwise, one would spend a lot of time only doing superfluous rebuilds.","title":"Caching"},{"location":"build-process-isolation/#context","text":"For the Development scenarios we need to have a context, for example the source code should be available on our machine, which would be shared of course with the container, however, ideally, such context is not mandatory for Building scenarios.","title":"Context"},{"location":"software/","text":"SW Source Capturing Issues Caution fixes Status deal2 AUR Done dune Done dumux Done eigen Arch + git repo Done espresso AUR Python-sphinx<2.1.0 is not found Building fails, follow https://aur.archlinux.org/packages/espressomd-git#comment-886320 cuda is to be installed, consuming 1.8GB of bandwidth and 6GB of disk space While installing, we had to correct the sha256sum of python-griddataformats in the aur Failed fenics Building fenics has some issues issue with build and install the C++ core \u00b7 Issue #2410 \u00b7 FEniCS/dolfinx \u00b7 GitHub Failed firedrake Ubuntu, build script https://github.com/firedrakeproject/firedrake/discussions/2596 Done gmsh AUR Done lammps Done palabos hdf5-openmpi was chosen for the installation https://gitlab.com/unigespc/palabos/-/issues/110, ssh is necessary for the first example Dependency on SSH is not stated (#111) \u00b7 Issues \u00b7 UniGeSPC / palabos \u00b7 GitLab Done petsc AUR Done precice AUR Done pyiron pymor su2 sundials trilinos visit xsdk ginkgo","title":"Software"},{"location":"software_bu/","text":"SW Source Capturing Issues Caution fixes Status deal2 AUR Done dune Done dumux Done eigen Arch + git repo Done espresso AUR Python-sphinx<2.1.0 is not found Building fails, follow https://aur.archlinux.org/packages/espressomd-git#comment-886320 cuda is to be installed, consuming 1.8GB of bandwidth and 6GB of disk space While installing, we had to correct the sha256sum of python-griddataformats in the aur Failed fenics Building fenics has some issues issue with build and install the C++ core \u00b7 Issue #2410 \u00b7 FEniCS/dolfinx \u00b7 GitHub Failed firedrake Ubuntu, build script https://github.com/firedrakeproject/firedrake/discussions/2596 Done gmsh AUR Done lammps Done palabos hdf5-openmpi was chosen for the installation https://gitlab.com/unigespc/palabos/-/issues/110, ssh is necessary for the first example Dependency on SSH is not stated (#111) \u00b7 Issues \u00b7 UniGeSPC / palabos \u00b7 GitLab petsc AUR precice AUR pyiron pymor su2 sundials trilinos visit xsdk ginkgo","title":"Software bu"},{"location":"sw-env-isolation/","text":"Software Environment Isolation \u00b6 In this isolation type, it\u2019s enough to have an image of the software binaries/libraries and we don\u2019t need at all to rebuild the software inside the docker image, as that would be: Error Prune, if the build is failed we would Violate the Separation of Concern, as we would now be only concerned about using the software. Not DRY, as we would be rebuilding the software every time the image caches are invalidated (for example: for updating a dependency) even though we already tested that with Building the Software isolation . Using the Software \u00b6 To isolate the SW usage we need to think first about how are the inputs and outputs of the software are formed, following are some samples: Inputs and outputs are arguments ./sw.sh input output Inputs are scripts that call the software echo testdata > input ./input Inputs are programs that get built and outputs are files (for example paraview plugins) cd inputDir echo testdata > input ./configure make Given these samples 2 different approaches could be considered each has his own tradeoff Note: The following 2 forms of isolating the software environment have the same concern thus they are considered 2 solutions for the same problem, taking into consideration that Adding a Backend Sidecar is a more sophisticated solution than Volume Sharing . And accordingly the number of forms mentioned in the intro section is only 3 not 4. Volume Sharing \u00b6 Through volume sharing we basically have a local directory (volume) that we develop over, which is shared with the container for both the inputs and outputs, that way the software with all it\u2019s libraries are running in an isolated environment. Note: the volume for the simulation output is optional as one could: 1. Use a single volume for input and output if the output is in the same directory 2. Copy the output directly to the host 3. The output could be a GUI based and directly appears to the user, for example by sharing the X11 Structure \u00b6 To implement this method, one needs to implement the following structure [user][Volume For Simulation Input] \u2192 [Docker Image With Binaries] \u2192 [Volume for Simulation Output (Optional)][user] Pros \u00b6 Pros of this method Simpler to implement for the DevOps engineer Cons \u00b6 Require containerization knowledge from the simulation engineer, which could be automated through make targets. Adding a Backend Sidecar \u00b6 To implement this method we would add a sidecar application to receive some input and return the output, for example using a RESTful API on TCP. The expected inputs are - Input data and simulation parameters - A script to execute over the input, that could take 2 forms - text form of the script: Which is very dangerous, but easier to extend - an option to select which pre-written script to execute: Safer, but would need to add scripts manually - A mix of previous options, supplying an option for the more privilege users to add scripts and less privilege users to select the option. Note: the script could be any of the previously mentioned 3 samples, or any other sample. As it basically an instruction set for the sidecar telling it how to run the simulation. Eventually the sidecar notifies the user when the output is ready, for example may by giving a status link for the user to check, then he would be able to download the output. In real life, most simulation software do not have such sidecar, or have it written in a non-standard way, which needs an adapter, and thus one would need to develop such sidecar, in an ideal scenario, generically to fit all supported SW. Structure \u00b6 To implement this method, one needs to implement the following structure [user][frontend] \u2194 [backend sidecar][Docker Image With Binaries] Pros \u00b6 Pros of this method Simpler for the simulation engineer, he needs only to upload and download files. Has more room for extensions, like multi-client servers Cons \u00b6 Cons of this method Requires understanding the protocol used for communication, or developing another frontend for uploading/downloading the software. Requires dependency on a sidecar. Requires the effort of developing and maintaining the sidecar. Caching \u00b6 If the sidecar implementation does not support caching and treats every simulation as a totally different entity, using the sidecar would be inefficient in running scenarios like the third sample, in which the input is to be built, thus caching reduces the time wasted for rebuilding unnecessary parts, which could be very expensive. Thus cache support would be considered a necessity for the sidecar to prove its worthiness in some cases. And of course, with such support, caching between different users would be a great addition that may make Adding a Backend Sidecar surpasses the Volume Sharing","title":"Software Environment Isolation"},{"location":"sw-env-isolation/#software-environment-isolation","text":"In this isolation type, it\u2019s enough to have an image of the software binaries/libraries and we don\u2019t need at all to rebuild the software inside the docker image, as that would be: Error Prune, if the build is failed we would Violate the Separation of Concern, as we would now be only concerned about using the software. Not DRY, as we would be rebuilding the software every time the image caches are invalidated (for example: for updating a dependency) even though we already tested that with Building the Software isolation .","title":"Software Environment Isolation"},{"location":"sw-env-isolation/#using-the-software","text":"To isolate the SW usage we need to think first about how are the inputs and outputs of the software are formed, following are some samples: Inputs and outputs are arguments ./sw.sh input output Inputs are scripts that call the software echo testdata > input ./input Inputs are programs that get built and outputs are files (for example paraview plugins) cd inputDir echo testdata > input ./configure make Given these samples 2 different approaches could be considered each has his own tradeoff Note: The following 2 forms of isolating the software environment have the same concern thus they are considered 2 solutions for the same problem, taking into consideration that Adding a Backend Sidecar is a more sophisticated solution than Volume Sharing . And accordingly the number of forms mentioned in the intro section is only 3 not 4.","title":"Using the Software"},{"location":"sw-env-isolation/#volume-sharing","text":"Through volume sharing we basically have a local directory (volume) that we develop over, which is shared with the container for both the inputs and outputs, that way the software with all it\u2019s libraries are running in an isolated environment. Note: the volume for the simulation output is optional as one could: 1. Use a single volume for input and output if the output is in the same directory 2. Copy the output directly to the host 3. The output could be a GUI based and directly appears to the user, for example by sharing the X11","title":"Volume Sharing"},{"location":"sw-env-isolation/#structure","text":"To implement this method, one needs to implement the following structure [user][Volume For Simulation Input] \u2192 [Docker Image With Binaries] \u2192 [Volume for Simulation Output (Optional)][user]","title":"Structure"},{"location":"sw-env-isolation/#pros","text":"Pros of this method Simpler to implement for the DevOps engineer","title":"Pros"},{"location":"sw-env-isolation/#cons","text":"Require containerization knowledge from the simulation engineer, which could be automated through make targets.","title":"Cons"},{"location":"sw-env-isolation/#adding-a-backend-sidecar","text":"To implement this method we would add a sidecar application to receive some input and return the output, for example using a RESTful API on TCP. The expected inputs are - Input data and simulation parameters - A script to execute over the input, that could take 2 forms - text form of the script: Which is very dangerous, but easier to extend - an option to select which pre-written script to execute: Safer, but would need to add scripts manually - A mix of previous options, supplying an option for the more privilege users to add scripts and less privilege users to select the option. Note: the script could be any of the previously mentioned 3 samples, or any other sample. As it basically an instruction set for the sidecar telling it how to run the simulation. Eventually the sidecar notifies the user when the output is ready, for example may by giving a status link for the user to check, then he would be able to download the output. In real life, most simulation software do not have such sidecar, or have it written in a non-standard way, which needs an adapter, and thus one would need to develop such sidecar, in an ideal scenario, generically to fit all supported SW.","title":"Adding a Backend Sidecar"},{"location":"sw-env-isolation/#structure_1","text":"To implement this method, one needs to implement the following structure [user][frontend] \u2194 [backend sidecar][Docker Image With Binaries]","title":"Structure"},{"location":"sw-env-isolation/#pros_1","text":"Pros of this method Simpler for the simulation engineer, he needs only to upload and download files. Has more room for extensions, like multi-client servers","title":"Pros"},{"location":"sw-env-isolation/#cons_1","text":"Cons of this method Requires understanding the protocol used for communication, or developing another frontend for uploading/downloading the software. Requires dependency on a sidecar. Requires the effort of developing and maintaining the sidecar.","title":"Cons"},{"location":"sw-env-isolation/#caching","text":"If the sidecar implementation does not support caching and treats every simulation as a totally different entity, using the sidecar would be inefficient in running scenarios like the third sample, in which the input is to be built, thus caching reduces the time wasted for rebuilding unnecessary parts, which could be very expensive. Thus cache support would be considered a necessity for the sidecar to prove its worthiness in some cases. And of course, with such support, caching between different users would be a great addition that may make Adding a Backend Sidecar surpasses the Volume Sharing","title":"Caching"}]}